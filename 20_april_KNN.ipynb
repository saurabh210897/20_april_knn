{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5efa82-bc65-4ad7-baf2-9d6a8aca093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?\n",
    "\n",
    "# Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "# Q4. How do you measure the performance of KNN?\n",
    "\n",
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "# Q6. How do you handle missing values in KNN?\n",
    "\n",
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for \n",
    "# which type of problem?\n",
    "\n",
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, \n",
    "# and how can these be addressed?\n",
    "\n",
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b13494-7467-4b3f-9dc5-0dae332414e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0ce314-0984-4f00-80cf-ae81afa01c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. \n",
    "# It is a non-parametric algorithm that makes predictions based on the similarity of data points.\n",
    "\n",
    "# In the KNN algorithm, the \"K\" refers to the number of nearest neighbors to consider. Given a new, unlabeled data point, \n",
    "# the algorithm finds the K nearest labeled data points (neighbors) in the training dataset based on a distance metric (usually Euclidean distance). \n",
    "# The most common approach is to calculate the distance between the new data point and all other points in the dataset.\n",
    "\n",
    "# For classification tasks, KNN assigns the majority class label among the K nearest neighbors to the new data point. In other words, \n",
    "# the class label of the majority of the neighbors determines the predicted class of the new data point.\n",
    "\n",
    "# For regression tasks, KNN predicts the value of the new data point based on the average (or weighted average) of the values of its K nearest neighbors. \n",
    "# This can be useful for predicting continuous or numeric values.\n",
    "\n",
    "# The choice of K is an important parameter in KNN. A small K value can lead to noisy predictions, while a large K value can result in over-generalization.\n",
    "# It's usually determined through cross-validation or other model evaluation techniques.\n",
    "\n",
    "# KNN is a simple and intuitive algorithm, but it can be computationally expensive, especially for large datasets. \n",
    "# It also assumes that nearby points are more likely to belong to the same class or have similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0aeb6a-4087-49db-aee0-362b1e82948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5435cc31-9754-4d99-9606-81a51d96b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is an important step in achieving accurate predictions. \n",
    "# The selection of K can have a significant impact on the algorithm's performance. Here are a few methods commonly used to determine the optimal value of K:\n",
    "\n",
    "# Domain knowledge and problem context: Understanding the problem you are trying to solve and having domain knowledge can provide insights into \n",
    "# selecting an appropriate value of K. For example, if you know that the decision boundary is complex and data points of different classes are intertwined,\n",
    "# you might choose a smaller value of K to capture local patterns.\n",
    "\n",
    "# Empirical testing and cross-validation: One common approach is to try different values of K and evaluate the algorithm's performance using cross-validation.\n",
    "# Cross-validation involves splitting the training data into multiple subsets (folds), training the model on a portion of the data,\n",
    "# and evaluating its performance on the remaining fold. By varying the value of K and observing the performance metrics (such as accuracy, precision, recall, \n",
    "# or mean squared error), you can identify the K value that provides the best results.\n",
    "\n",
    "# Odd value for binary classification: When dealing with binary classification problems, it's often recommended to choose an odd value for K to avoid ties\n",
    "# when determining the majority class. Having an odd K ensures that there won't be an equal number of neighbors from each class, allowing for a clear decision.\n",
    "\n",
    "# Square root rule: The square root of the total number of data points in the training set is a rule of thumb that is sometimes used to determine the value of K. \n",
    "# For example, if you have 100 training examples, the square root of 100 is 10, so you might choose K=10.\n",
    "\n",
    "# Algorithm performance trade-off: The choice of K involves a trade-off between bias and variance. A small value of K (e.g., K=1) leads to low bias but high variance, \n",
    "# meaning the model can overfit the training data. On the other hand, a large value of K (e.g., K=20) introduces high bias but low variance,\n",
    "# potentially leading to underfitting. You can experiment with different K values to strike the right balance based on your specific dataset and problem.\n",
    "\n",
    "# It's important to note that the optimal value of K can vary depending on the dataset and problem at hand. Therefore, it's recommended to try multiple values of K \n",
    "# and assess their impact on the algorithm's performance before making a final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a118de5e-d518-48a7-911a-8887a1102cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9397c3db-f6a4-4390-8aa2-26f01420438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The main difference between the K-Nearest Neighbors (KNN) classifier and KNN regressor lies in the type of prediction they make and the nature of the target variable.\n",
    "\n",
    "# KNN Classifier: The KNN classifier is used for classification tasks, where the target variable is categorical or discrete.\n",
    "# It assigns class labels to unlabeled data points based on the majority class of their K nearest neighbors.\n",
    "# The predicted class label is determined by voting or taking the mode of the classes of the K nearest neighbors. \n",
    "# For example, in a binary classification problem, the KNN classifier would assign a new data point to either class A or class B \n",
    "# based on the majority vote of its K nearest neighbors.\n",
    "\n",
    "# KNN Regressor: The KNN regressor, on the other hand, is used for regression tasks, where the target variable is continuous or numeric. \n",
    "# Instead of predicting class labels, the KNN regressor predicts the value of the target variable for unlabeled data points based on the average\n",
    "# (or weighted average) of the values of their K nearest neighbors. For example, in a regression problem predicting housing prices,\n",
    "# the KNN regressor would estimate the price of a new house by averaging the prices of its K nearest neighbors.\n",
    "\n",
    "# In both cases, the KNN algorithm calculates the similarity between data points using a distance metric (usually Euclidean distance)\n",
    "# to determine the nearest neighbors. The value of K is a parameter that determines the number of neighbors considered for prediction.\n",
    "\n",
    "# In summary, the KNN classifier is used for categorical classification tasks, while the KNN regressor is used for continuous regression tasks. \n",
    "# The KNN classifier predicts class labels based on majority voting, while the KNN regressor predicts the numeric value based on averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93c1626-862e-4d78-80b8-1f8ecbe90fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3891d809-5cee-4874-9f5c-0759817a8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To measure the performance of the K-Nearest Neighbors (KNN) algorithm, several evaluation metrics can be utilized depending on the specific task at hand \n",
    "# (classification or regression). Here are some commonly used performance measures for KNN:\n",
    "\n",
    "# For Classification:\n",
    "\n",
    "# Accuracy: Accuracy is the most straightforward metric, representing the proportion of correctly classified instances over the total number of instances. \n",
    "# It provides an overall measure of the model's predictive power.\n",
    "\n",
    "# Precision and Recall: Precision measures the proportion of correctly predicted positive instances among all instances predicted as positive. \n",
    "# Recall (also called sensitivity or true positive rate) measures the proportion of correctly predicted positive instances among all actual positive instances. \n",
    "# These metrics are especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "# F1-Score: The F1-score is the harmonic mean of precision and recall, providing a single measure that balances both metrics. \n",
    "# It is useful when there is an uneven distribution between classes or when both precision and recall are important.\n",
    "\n",
    "# Confusion Matrix: A confusion matrix provides a more detailed analysis of the model's performance, showing the counts of true positives, true negatives,\n",
    "# false positives, and false negatives. From the confusion matrix, metrics such as precision, recall, and accuracy can be derived.\n",
    "\n",
    "# For Regression:\n",
    "\n",
    "# Mean Squared Error (MSE): MSE measures the average squared difference between the predicted and actual values. It penalizes larger errors more heavily, \n",
    "# making it sensitive to outliers. Lower MSE values indicate better performance.\n",
    "\n",
    "# Mean Absolute Error (MAE): MAE calculates the average absolute difference between the predicted and actual values. \n",
    "# It provides a measure of the average magnitude of the errors, regardless of their direction.\n",
    "\n",
    "# R-squared (Coefficient of Determination): R-squared represents the proportion of the variance in the target variable that is explained by the model. \n",
    "# It ranges from 0 to 1, with higher values indicating a better fit. However, R-squared may not be the most reliable metric for KNN regression as it assumes linearity.\n",
    "\n",
    "# It's important to note that the choice of evaluation metric depends on the specific problem and the priorities of the analysis. \n",
    "# It's recommended to consider multiple metrics to gain a comprehensive understanding of the KNN algorithm's performance. \n",
    "# Additionally, cross-validation techniques such as k-fold cross-validation can be employed to estimate the performance on unseen data and reduce bias in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1fca7cb-ad49-437c-98ba-a83b4654a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6063842-3043-459e-a868-78a99aa0a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The \"curse of dimensionality\" refers to the challenges and issues that arise when working with high-dimensional data in machine learning algorithms,\n",
    "# including the K-Nearest Neighbors (KNN) algorithm. It describes the phenomena where the performance and efficiency of certain algorithms degrade significantly\n",
    "# as the number of dimensions or features increases.\n",
    "\n",
    "# In the context of KNN, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "# Increased computational complexity: As the number of dimensions increases, the number of calculations required to compute distances between data points\n",
    "# grows exponentially. This leads to increased computational time and memory requirements, making the algorithm slower and less efficient.\n",
    "\n",
    "# Sparsity of data: In high-dimensional spaces, data becomes sparser. As the number of dimensions increases, the available data points become more spread out,\n",
    "# making it more challenging to find close neighbors. This can lead to less reliable or less representative nearest neighbors.\n",
    "\n",
    "# Increased similarity of data points: Paradoxically, as the number of dimensions increases, the distance between any two data points tends to become more similar.\n",
    "# In other words, most pairs of data points become equidistant from each other, which can result in less discriminative power and difficulty in distinguishing between \n",
    "# different classes or patterns.\n",
    "\n",
    "# Overfitting: With high-dimensional data, there is a higher risk of overfitting the model to noise or outliers in the data.\n",
    "# KNN can become sensitive to irrelevant features, leading to poor generalization and performance on unseen data.\n",
    "\n",
    "# To mitigate the curse of dimensionality in KNN, several techniques can be employed:\n",
    "\n",
    "# a. Feature selection or dimensionality reduction: Selecting relevant features or reducing the dimensionality of the data can help improve the performance \n",
    "# and computational efficiency of KNN. Techniques like Principal Component Analysis (PCA) or feature selection algorithms can be used to identify \n",
    "# the most informative features.\n",
    "\n",
    "# b. Distance metrics: Choosing appropriate distance metrics tailored to the characteristics of the data can help alleviate the curse of dimensionality.\n",
    "# For example, using specialized distance measures like Mahalanobis distance that consider covariance structure or incorporating feature weighting can be beneficial.\n",
    "\n",
    "# c. Data preprocessing techniques: Applying data preprocessing techniques such as normalization or standardization can help to normalize the scale of \n",
    "# different features and reduce the impact of varying feature ranges.\n",
    "\n",
    "# d. Data augmentation: In some cases, generating additional synthetic data points or using data augmentation techniques can help address the sparsity issue in\n",
    "# high-dimensional spaces.\n",
    "\n",
    "# Overall, it's crucial to consider the curse of dimensionality when working with high-dimensional data in KNN and other machine learning algorithms. \n",
    "# Proper preprocessing, dimensionality reduction, and feature selection strategies can help mitigate the negative effects and improve the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afcc493-1c72-4931-b70f-2be1c86791dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eae01f5-27d1-4d4f-a7e6-62f7da5c554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values in the K-Nearest Neighbors (KNN) algorithm requires careful consideration, as the algorithm relies on the similarity between data points\n",
    "# to make predictions. Here are a few approaches to address missing values in KNN:\n",
    "\n",
    "# Deletion of instances: If the dataset has a small proportion of missing values, you can choose to remove instances (rows) with missing values from the dataset. \n",
    "# However, this approach may result in a loss of valuable information, especially if the missing values are not randomly distributed.\n",
    "\n",
    "# Deletion of features: If a particular feature (column) has a high percentage of missing values or is deemed irrelevant, it can be removed from the dataset.\n",
    "# This can be a reasonable approach if the feature does not contribute significantly to the predictive power of the model.\n",
    "\n",
    "# Mean/Median/Mode imputation: In this approach, missing values are replaced with the mean (for numerical data), median, \n",
    "# or mode (for categorical data) of the respective feature. This method assumes that the missing values are missing at random\n",
    "# and can introduce bias if the missingness is related to the target variable.\n",
    "\n",
    "# KNN imputation: Instead of removing instances with missing values, KNN imputation fills in the missing values based on the values of the nearest neighbors. \n",
    "# The algorithm identifies the K nearest neighbors based on the available features and uses their values to impute the missing values.\n",
    "# The average (or weighted average) of the nearest neighbors' values can be used for numerical features, while the most frequent value can be used for categorical \n",
    "# features.\n",
    "\n",
    "# Advanced imputation techniques: There are more sophisticated imputation techniques that can be employed, such as multiple imputation or regression-based imputation, \n",
    "# which use additional relationships between variables to estimate missing values more accurately. \n",
    "# These methods take into account the correlations between variables and provide more robust imputations.\n",
    "\n",
    "# It's important to note that the choice of missing data handling technique depends on the specific dataset and the nature of the missing values.\n",
    "# Each approach has its assumptions and limitations, and it's recommended to evaluate the impact of missing data handling techniques on the performance of \n",
    "# the KNN algorithm through cross-validation or other evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "990a8d7a-7530-43cd-8a8a-3d701dca6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for \n",
    "# which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f2f144-1863-49fa-ad50-0e3c5169a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The performance of the K-Nearest Neighbors (KNN) classifier and regressor can vary based on the problem at hand.\n",
    "# Here's a comparison between the two and guidance on which one may be more suitable for specific types of problems:\n",
    "\n",
    "# Prediction Task:\n",
    "\n",
    "# KNN Classifier: The KNN classifier is suitable for classification tasks where the target variable is categorical or discrete.\n",
    "# It predicts class labels for unlabeled data points based on the majority class of their K nearest neighbors.\n",
    "# KNN Regressor: The KNN regressor is appropriate for regression tasks where the target variable is continuous or numeric. \n",
    "# It predicts the value of the target variable for unlabeled data points based on the average (or weighted average) of their K nearest neighbors.\n",
    "# Data Type:\n",
    "\n",
    "# KNN Classifier: The KNN classifier works well with categorical or discrete data as it is designed to classify data into different classes. \n",
    "# It can handle binary classification (two classes) or multi-class classification (more than two classes).\n",
    "# KNN Regressor: The KNN regressor is suitable for working with continuous or numeric data, making it ideal for predicting numeric values such as prices,\n",
    "# temperatures, or quantities.\n",
    "# Decision Boundary:\n",
    "\n",
    "# KNN Classifier: The decision boundary of the KNN classifier is non-linear and can be flexible, making it effective for capturing complex decision boundaries \n",
    "# and handling problems with non-linear separability.\n",
    "# KNN Regressor: The KNN regressor does not explicitly define decision boundaries. Instead, \n",
    "# it predicts continuous values based on the average of neighboring data points. \n",
    "# The decision surface is not explicitly defined but can be visualized as a smooth surface based on the interpolated values.\n",
    "# Interpretability:\n",
    "\n",
    "# KNN Classifier: The KNN classifier provides interpretability by indicating the majority class label among the K nearest neighbors. \n",
    "# It allows understanding of the importance and influence of individual neighbors on the classification decision.\n",
    "# KNN Regressor: The KNN regressor provides interpretability by providing insight into the average values of neighboring data points and their influence on \n",
    "# the predicted value. It allows understanding the contribution of individual neighbors to the regression result.\n",
    "\n",
    "# In summary, the choice between KNN classifier and regressor depends on the problem type and the nature of the target variable. \n",
    "# If the target variable is categorical, and the goal is to classify data into classes, then the KNN classifier is appropriate. \n",
    "# On the other hand, if the target variable is continuous or numeric, and the goal is to predict numeric values, then the KNN regressor is more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c18dfb-d91f-4642-9e33-a68ef1005163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, \n",
    "# and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de023253-93a6-4cb9-981d-860d567459ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The K-Nearest Neighbors (KNN) algorithm has its own strengths and weaknesses for both classification and regression tasks. \n",
    "# Understanding these aspects can help in addressing their limitations and maximizing their benefits:\n",
    "\n",
    "# Strengths of KNN:\n",
    "\n",
    "# Simplicity: KNN is a simple and intuitive algorithm that is easy to understand and implement. It serves as a good baseline model for classification \n",
    "# and regression tasks.\n",
    "\n",
    "# Non-parametric: KNN is a non-parametric algorithm, meaning it does not make any assumptions about the underlying data distribution. \n",
    "# It can handle complex patterns and adapt to various types of data.\n",
    "\n",
    "# Flexibility in decision boundaries: KNN can capture non-linear decision boundaries, making it effective for problems with complex and non-linear separability.\n",
    "\n",
    "# Instance-based learning: KNN does not build an explicit model during the training phase, but rather memorizes the instances in the training data.\n",
    "# This makes it useful in scenarios where the data distribution is not static and changes over time.\n",
    "\n",
    "# Weaknesses of KNN:\n",
    "\n",
    "# Computational complexity: The computational cost of KNN grows with the size of the training data and the number of dimensions. \n",
    "# Calculating distances between data points can be time-consuming,\n",
    "# especially for large datasets and high-dimensional feature spaces.\n",
    "\n",
    "# Sensitive to feature scaling: KNN calculates distances based on the features' values. If the features have different scales,\n",
    "# those with larger magnitudes can dominate the distance calculation, leading to biased results. Feature scaling techniques,\n",
    "# such as normalization or standardization, can address this issue.\n",
    "\n",
    "# Curse of dimensionality: As the number of dimensions increases, the available data becomes sparse, and the similarity between data points decreases.\n",
    "# This can degrade the performance of KNN and lead to inaccurate predictions. Dimensionality reduction techniques or feature selection methods can \n",
    "# help mitigate this issue.\n",
    "\n",
    "# Imbalanced class distribution: KNN can be biased towards the majority class in imbalanced datasets. This can result in poor performance for minority classes. \n",
    "# Techniques like oversampling, undersampling, or using weighted distance metrics can help address this imbalance.\n",
    "\n",
    "# Addressing Weaknesses:\n",
    "\n",
    "# Efficient data structures: Using efficient data structures like KD-trees or Ball trees can speed up the process of finding nearest neighbors, \n",
    "# reducing computational complexity.\n",
    "\n",
    "# Feature selection and dimensionality reduction: Selecting relevant features or reducing the dimensionality of the data can improve the performance\n",
    "# and computational efficiency of KNN.\n",
    "\n",
    "# Distance metric selection: Choosing an appropriate distance metric tailored to the specific problem can improve the algorithm's performance.\n",
    "# Different distance metrics, such as Euclidean distance, Manhattan distance, or cosine similarity, may be more suitable for different data types.\n",
    "\n",
    "# Model tuning: Experimenting with different values of K and evaluating performance metrics can help find the optimal value for a specific problem. \n",
    "# Cross-validation techniques can be used to estimate the model's performance on unseen data.\n",
    "\n",
    "# Ensemble methods: Combining multiple KNN models with different hyperparameters or using ensemble methods like bagging or boosting can enhance \n",
    "\n",
    "# the performance and robustness of KNN.\n",
    "# By considering these strengths and weaknesses of the KNN algorithm and employing appropriate techniques, it is possible to address its limitations\n",
    "# and leverage its strengths to achieve better performance in classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d08c310-7a9d-4cbf-8cbf-518f55a9272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f9fbb1-3035-4601-9382-7f8825a98801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Euclidean distance and Manhattan distance are two commonly used distance metrics in the K-Nearest Neighbors (KNN) algorithm. \n",
    "# Here are the key differences between them:\n",
    "\n",
    "# Euclidean Distance: The Euclidean distance between two points in a multidimensional space is the straight-line distance between them, \n",
    "# measured as the square root of the sum of the squared differences between their coordinates. In other words,\n",
    "# it calculates the length of the vector connecting the two points. Mathematically, the Euclidean distance between \n",
    "# two points (x1, y1, z1, ...) and (x2, y2, z2, ...) in a D-dimensional space is given by:\n",
    "\n",
    "# Euclidean Distance Formula\n",
    "\n",
    "# Euclidean distance considers both the magnitude and direction of the differences between coordinates. It works well when the underlying data follows \n",
    "# a continuous and smooth distribution.\n",
    "\n",
    "# Manhattan Distance: The Manhattan distance, also known as the city block distance or L1 distance, calculates the distance between two points \n",
    "# by summing the absolute differences of their coordinates. It represents the shortest path a person could take when moving only in horizontal\n",
    "# and vertical directions on a grid-like structure. Mathematically, the Manhattan distance between two points (x1, y1, z1, ...)\n",
    "# and (x2, y2, z2, ...) in a D-dimensional space is given by:\n",
    "\n",
    "# Manhattan Distance Formula\n",
    "\n",
    "# Unlike the Euclidean distance, the Manhattan distance does not consider the actual distance or straight-line path between two points. \n",
    "# It only considers the sum of the absolute differences along each dimension. Manhattan distance is often used when dealing\n",
    "# with data that follows a grid-like or lattice-like structure.\n",
    "\n",
    "# Key Differences:\n",
    "\n",
    "# Euclidean distance considers both magnitude and direction, while Manhattan distance only considers magnitude.\n",
    "# Euclidean distance calculates the straight-line distance, while Manhattan distance calculates the distance based on horizontal and vertical movements.\n",
    "# Euclidean distance is suitable for continuous and smooth data distributions, while Manhattan distance is suitable for grid-like or lattice-like data distributions.\n",
    "# Euclidean distance is more sensitive to outliers than Manhattan distance since it considers the squared differences.\n",
    "# In high-dimensional spaces, Manhattan distance tends to be less affected by the curse of dimensionality compared to Euclidean distance.\n",
    "\n",
    "# The choice between Euclidean distance and Manhattan distance depends on the nature of the data and the problem at hand. \n",
    "# It is recommended to experiment with both distance metrics and assess their impact on the KNN algorithm's performance to determine the most suitable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a2d921e-cbf9-487c-8eee-9d9f77c96ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927f363c-1708-482d-848a-4c5583b4fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, feature scaling ensures that all features contribute equally to the distance calculation in KNN. \n",
    "# It improves the fairness of feature comparisons and can lead to better performance and more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5c61e-93eb-4fb5-870e-22a84d797e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
